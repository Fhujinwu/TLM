<h1 align="center">
     <br>Test-Time Learning for Large Language Models
<p align="center">
    <a href="https://arxiv.org/pdf/2505.20633">
        <img alt="Static Badge" src="https://img.shields.io/badge/Paper-Arxiv-red">
    </a>
    <a href="https://huggingface.co/">
        <img alt="Static Badge" src="https://img.shields.io/badge/HFDataset-TLM-yellow">
    </a>
</p>

## 🔥News
- *2025-05-27*: We have released our paper on Arxiv.
- *2025-05-01*: TLM is accepted by ICML2025.

## 🚀Quick Start 
```bash
## clone our repo
git clone https://github.com/Fhujinwu/TLM.git
cd TLM
## install TLM environment
conda create --name tlm --yes python=3.11
conda activate tlm
pip install -e ".[torch,metrics]" --no-build-isolation
```
## 🗂 Data Selection
TODO

## 🔨 Training
TODO

## ⚖️ Evaluation
TODO

## 💬 Citation
Thanks for the open-source code of [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

If you find our work interesting and meaningful, welcome to give a 🌟 to our repo and cite our paper.

```text
@inproceedings{hu2025tlm,
      title={Test-Time Learning for Large Language Models}, 
      author={Jinwu Hu and Zhitian Zhang and Guohao Chen and Xutao Wen and Chao Shuai and Wei Luo and Bin Xiao and Yuanqing Li and Mingkui Tan},
      booktitle={International conference on machine learning},
      year={2025},
      organization={PMLR}
}
```

## Star History

![Star History Chart](https://api.star-history.com/svg?repos=Fhujinwu/TLM&type=Date)
